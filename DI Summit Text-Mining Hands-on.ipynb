{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![B&D](http://www.avenir-it.fr/wp-content/uploads/2015/10/BD-Logo-groupe.jpg)\n",
    "\n",
    "# Demo text-mining: Pharma case\n",
    "\n",
    "In this demo, I will demonstrate what are the basic steps that you will have to use in most text-mining cases. This are also some of the steps that have been used in the ResuMe app, Giulia just showed you. The case that we will cover here, is a simplified version of a project that has actually been carried out by Radia, where the goal was to identify if a given paper is treating about Pharmacovigilance or not. Pharmacovigilance is a domain of study in healthcare about drug safety. Consequently, we would like to predict, based on the text of the scientific article if the article treats about Pharmacovigilance or not.\n",
    "\n",
    "For this we can use any kind of model, but in any case we will have to transform the words in numbers in some way. We'll see different methods and compare their performance.\n",
    "\n",
    "## Downloading the dataset from PubMed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-26T10:51:58.023383Z",
     "start_time": "2018-06-26T10:51:57.997817Z"
    }
   },
   "outputs": [],
   "source": [
    "#import documents from PubMed\n",
    "from Bio import Entrez\n",
    "\n",
    "# Function to search for a certain number articles based on a certain keyword\n",
    "def search(keyword,number=20):\n",
    "    Entrez.email = 'your.email@example.com'\n",
    "    handle = Entrez.esearch(db='pubmed', \n",
    "                            sort='relevance', \n",
    "                            retmax=str(number),\n",
    "                            retmode='xml', \n",
    "                            term=keyword)\n",
    "    results = Entrez.read(handle)\n",
    "    return results\n",
    "\n",
    "# Function to retrieve the results of previous search query\n",
    "def fetch_details(id_list):\n",
    "    ids = ','.join(id_list)\n",
    "    Entrez.email = 'your.email@example.com'\n",
    "    handle = Entrez.efetch(db='pubmed',\n",
    "                           retmode='xml',\n",
    "                           id=ids)\n",
    "    results = Entrez.read(handle)\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieving top 200 articles with Pharmacovigilance keyword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-26T10:52:31.760292Z",
     "start_time": "2018-06-26T10:52:27.942977Z"
    }
   },
   "outputs": [],
   "source": [
    "results = search('Pharmacovigilance', 200) #querying PubMed\n",
    "id_list = results['IdList']\n",
    "papers_pharmacov = fetch_details(id_list) #retrieving the info about the articles in nested lists & dictionary format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-26T10:52:31.777007Z",
     "start_time": "2018-06-26T10:52:31.762493Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1) FarmaREL: An Italian pharmacovigilance project to monitor and evaluate adverse drug reactions in haematologic patients.\n",
      "2) Feasibility and Educational Value of a Student-Run Pharmacovigilance Programme: A Prospective Cohort Study.\n",
      "3) Developing a Crowdsourcing Approach and Tool for Pharmacovigilance Education Material Delivery.\n",
      "4) Promoting and Protecting Public Health: How the European Union Pharmacovigilance System Works.\n",
      "5) Effect of an educational intervention on knowledge and attitude regarding pharmacovigilance and consumer pharmacovigilance among community pharmacists in Lalitpur district, Nepal.\n",
      "6) Pharmacovigilance and Biomedical Informatics: A Model for Future Development.\n",
      "7) Pharmacovigilance in Europe: Place of the Pharmacovigilance Risk Assessment Committee (PRAC) in organisation and decisional processes.\n",
      "8) Tamoxifen Pharmacovigilance: Implications for Safe Use in the Future.\n",
      "9) Pharmacovigilance Skills, Knowledge and Attitudes in our Future Doctors - A Nationwide Study in the Netherlands.\n",
      "10) Adverse drug reactions reporting in Calabria (Southern Italy) in the four-year period 2011-2014: impact of a regional pharmacovigilance project in light of the new European Legislation.\n"
     ]
    }
   ],
   "source": [
    "# checking article title for the first 10 retrieved articles\n",
    "for i, paper in enumerate(papers_pharmacov['PubmedArticle'][:10]):\n",
    "        print(\"%d) %s\" % (i+1,paper['MedlineCitation']['Article']['ArticleTitle']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieving top 1.000 articles with Pharma keyword\n",
    "This will be our base of comparison, we want to separate them from the others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-26T10:52:48.091619Z",
     "start_time": "2018-06-26T10:52:37.856713Z"
    }
   },
   "outputs": [],
   "source": [
    "results = search('Pharma', 1000) #querying PubMed\n",
    "id_list = results['IdList']\n",
    "papers_pharma = fetch_details(id_list)#retrieving the info about the articles in nested lists & dictionary format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-26T10:52:48.123700Z",
     "start_time": "2018-06-26T10:52:48.097630Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1) Recent trends in specialty pharma business model.\n",
      "2) The moderating role of absorptive capacity and the differential effects of acquisitions and alliances on Big Pharma firms' innovation performance.\n",
      "3) Space-related pharma-motifs for fast search of protein binding motifs and polypharmacological targets.\n",
      "4) Pharma Websites and \"Professionals-Only\" Information: The Implications for Patient Trust and Autonomy.\n",
      "5) BRIC Health Systems and Big Pharma: A Challenge for Health Policy and Management.\n",
      "6) Developing Deep Learning Applications for Life Science and Pharma Industry.\n",
      "7) Exzellenz in der Bildung für eine innovative Schweiz: Die Position des Wirtschaftsdachverbandes Chemie Pharma Biotech.\n",
      "8) Shaking Up Biotech/Pharma: Can Cues Be Taken from the Tech Industry?\n",
      "9) Pharma-Nutritional Properties of Olive Oil Phenols. Transfer of New Findings to Human Nutrition.\n",
      "10) Pharma Success in Product Development—Does Biotechnology Change the Paradigm in Product Development and Attrition.\n"
     ]
    }
   ],
   "source": [
    "# checking article title for the first 10 retrieved articles\n",
    "for i, paper in enumerate(papers_pharma['PubmedArticle'][:10]):\n",
    "        print(\"%d) %s\" % (i+1,paper['MedlineCitation']['Article']['ArticleTitle']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving ID's, labels and title + abstracts of the articles\n",
    "\n",
    "When an article was retrieved via the Pharmacovigilance keyword, it will receive the label = 1 and = 0 else. We'll per article put the article title and article abstract together as our text data on the article. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-26T10:53:42.379359Z",
     "start_time": "2018-06-26T10:53:42.341758Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Save ids & label 1 = pharmacovigilance , 0 =  not pharmacovigilance\n",
    "# & Save title + abstract in  dico\n",
    "ids = []\n",
    "labels = []\n",
    "data = []\n",
    "for i, paper in enumerate(papers_pharmacov['PubmedArticle']):\n",
    "    if 'Abstract' in paper['MedlineCitation']['Article'].keys(): #check that abstract info is available\n",
    "        ids.append(str(paper['MedlineCitation']['PMID']))\n",
    "        labels.append(1)\n",
    "        title = paper['MedlineCitation']['Article']['ArticleTitle'] #Article title\n",
    "        abstract = paper['MedlineCitation']['Article']['Abstract']['AbstractText'][0] #Abstract\n",
    "        data.append( title + abstract )\n",
    "for i, paper in enumerate(papers_pharma['PubmedArticle']):\n",
    "    if 'Abstract' in paper['MedlineCitation']['Article'].keys(): #check that abstract info is available\n",
    "        ids.append(str(paper['MedlineCitation']['PMID']))\n",
    "        labels.append(0)\n",
    "        title = paper['MedlineCitation']['Article']['ArticleTitle'] #Article title\n",
    "        abstract = paper['MedlineCitation']['Article']['Abstract']['AbstractText'][0] #Abstract\n",
    "        data.append( title + abstract )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-26T10:53:49.350388Z",
     "start_time": "2018-06-26T10:53:49.329835Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'28771763'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'FarmaREL: An Italian pharmacovigilance project to monitor and evaluate adverse drug reactions in haematologic patients.Adverse drug reactions (ADRs) reduce patients\\' quality of life, increase mortality and morbidity, and have a negative economic impact on healthcare systems. Nevertheless, the importance of ADR reporting is often underestimated. The project \"FarmaREL\" has been developed to monitor and evaluate ADRs in haematological patients and to increase pharmacovigilance culture among haematology specialists. In 13 haematology units, based in Lombardy, Italy, a dedicated specialist with the task of encouraging ADRs reporting and sensitizing healthcare professionals to pharmacovigilance has been assigned. The ADRs occurring in haematological patients were collected electronically and then analysed with multiple logistic regression. Between January 2009 and December 2011, 887 reports were collected. The number of ADRs was higher in older adults (528; 59%), in male (490; 55%), and in non-Hodgkin lymphoma patients (343; 39%). Most reactions were severe (45% required or prolonged hospitalization), but in most cases, they were fully resolved at the time of reporting. According to Schumock and Thornton criteria, a percentage of ADRs as high as 7% was found to be preventable versus 2% according to reporter opinion. Patients\\' haematological diagnosis, not age or gender, resulted to be the variable that most influenced ADR, in particular severity and outcome. The employment of personnel specifically dedicated to pharmacovigilance is a successful strategy to improve the number and quality of ADR reports. \"FarmaREL\", the first programme of active pharmacovigilance in oncohaematologic patients, significantly contributed to reach the WHO \"Gold Standard\" for pharmacovigilance in Lombardy, Italy.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check result for one paper\n",
    "ids[0] # ID\n",
    "labels[0] # 1 = pharmacovigilance , 0 =  not pharmacovigilance\n",
    "data[0] # Title & abstract"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform to numeric attributes\n",
    "We will now **transform** the **text into numeric attributes**. For this, we will convert every word to a number, but we first need to **split** the full text into **separate words**. This is done by using a ***Tokenizer***. The tokenizer will split the full text based on a certain pattern you specify. Here we'll take a very basic pattern and take any words that contain only upper- or lowercase letters and we will convert everything to lowercase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-26T10:55:17.414174Z",
     "start_time": "2018-06-26T10:55:16.506557Z"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize.regexp import RegexpTokenizer #import a tokenizer, to split the full text into separate words\n",
    "\n",
    "def Tokenize_text_value(value):\n",
    "    tokenizer1 = RegexpTokenizer(r\"[A-Za-z]+\")  # our self defined tokenizera\n",
    "    value = value.lower()  # convert all words to lowercase\n",
    "    return tokenizer1.tokenize(value)  # tokenize each text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-26T10:55:18.371779Z",
     "start_time": "2018-06-26T10:55:18.349217Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['farmarel',\n",
       " 'an',\n",
       " 'italian',\n",
       " 'pharmacovigilance',\n",
       " 'project',\n",
       " 'to',\n",
       " 'monitor',\n",
       " 'and',\n",
       " 'evaluate',\n",
       " 'adverse',\n",
       " 'drug',\n",
       " 'reactions',\n",
       " 'in',\n",
       " 'haematologic',\n",
       " 'patients',\n",
       " 'adverse',\n",
       " 'drug',\n",
       " 'reactions',\n",
       " 'adrs',\n",
       " 'reduce',\n",
       " 'patients',\n",
       " 'quality',\n",
       " 'of',\n",
       " 'life',\n",
       " 'increase',\n",
       " 'mortality',\n",
       " 'and',\n",
       " 'morbidity',\n",
       " 'and',\n",
       " 'have',\n",
       " 'a',\n",
       " 'negative',\n",
       " 'economic',\n",
       " 'impact',\n",
       " 'on',\n",
       " 'healthcare',\n",
       " 'systems',\n",
       " 'nevertheless',\n",
       " 'the',\n",
       " 'importance',\n",
       " 'of',\n",
       " 'adr',\n",
       " 'reporting',\n",
       " 'is',\n",
       " 'often',\n",
       " 'underestimated',\n",
       " 'the',\n",
       " 'project',\n",
       " 'farmarel',\n",
       " 'has',\n",
       " 'been',\n",
       " 'developed',\n",
       " 'to',\n",
       " 'monitor',\n",
       " 'and',\n",
       " 'evaluate',\n",
       " 'adrs',\n",
       " 'in',\n",
       " 'haematological',\n",
       " 'patients',\n",
       " 'and',\n",
       " 'to',\n",
       " 'increase',\n",
       " 'pharmacovigilance',\n",
       " 'culture',\n",
       " 'among',\n",
       " 'haematology',\n",
       " 'specialists',\n",
       " 'in',\n",
       " 'haematology',\n",
       " 'units',\n",
       " 'based',\n",
       " 'in',\n",
       " 'lombardy',\n",
       " 'italy',\n",
       " 'a',\n",
       " 'dedicated',\n",
       " 'specialist',\n",
       " 'with',\n",
       " 'the',\n",
       " 'task',\n",
       " 'of',\n",
       " 'encouraging',\n",
       " 'adrs',\n",
       " 'reporting',\n",
       " 'and',\n",
       " 'sensitizing',\n",
       " 'healthcare',\n",
       " 'professionals',\n",
       " 'to',\n",
       " 'pharmacovigilance',\n",
       " 'has',\n",
       " 'been',\n",
       " 'assigned',\n",
       " 'the',\n",
       " 'adrs',\n",
       " 'occurring',\n",
       " 'in',\n",
       " 'haematological',\n",
       " 'patients',\n",
       " 'were',\n",
       " 'collected',\n",
       " 'electronically',\n",
       " 'and',\n",
       " 'then',\n",
       " 'analysed',\n",
       " 'with',\n",
       " 'multiple',\n",
       " 'logistic',\n",
       " 'regression',\n",
       " 'between',\n",
       " 'january',\n",
       " 'and',\n",
       " 'december',\n",
       " 'reports',\n",
       " 'were',\n",
       " 'collected',\n",
       " 'the',\n",
       " 'number',\n",
       " 'of',\n",
       " 'adrs',\n",
       " 'was',\n",
       " 'higher',\n",
       " 'in',\n",
       " 'older',\n",
       " 'adults',\n",
       " 'in',\n",
       " 'male',\n",
       " 'and',\n",
       " 'in',\n",
       " 'non',\n",
       " 'hodgkin',\n",
       " 'lymphoma',\n",
       " 'patients',\n",
       " 'most',\n",
       " 'reactions',\n",
       " 'were',\n",
       " 'severe',\n",
       " 'required',\n",
       " 'or',\n",
       " 'prolonged',\n",
       " 'hospitalization',\n",
       " 'but',\n",
       " 'in',\n",
       " 'most',\n",
       " 'cases',\n",
       " 'they',\n",
       " 'were',\n",
       " 'fully',\n",
       " 'resolved',\n",
       " 'at',\n",
       " 'the',\n",
       " 'time',\n",
       " 'of',\n",
       " 'reporting',\n",
       " 'according',\n",
       " 'to',\n",
       " 'schumock',\n",
       " 'and',\n",
       " 'thornton',\n",
       " 'criteria',\n",
       " 'a',\n",
       " 'percentage',\n",
       " 'of',\n",
       " 'adrs',\n",
       " 'as',\n",
       " 'high',\n",
       " 'as',\n",
       " 'was',\n",
       " 'found',\n",
       " 'to',\n",
       " 'be',\n",
       " 'preventable',\n",
       " 'versus',\n",
       " 'according',\n",
       " 'to',\n",
       " 'reporter',\n",
       " 'opinion',\n",
       " 'patients',\n",
       " 'haematological',\n",
       " 'diagnosis',\n",
       " 'not',\n",
       " 'age',\n",
       " 'or',\n",
       " 'gender',\n",
       " 'resulted',\n",
       " 'to',\n",
       " 'be',\n",
       " 'the',\n",
       " 'variable',\n",
       " 'that',\n",
       " 'most',\n",
       " 'influenced',\n",
       " 'adr',\n",
       " 'in',\n",
       " 'particular',\n",
       " 'severity',\n",
       " 'and',\n",
       " 'outcome',\n",
       " 'the',\n",
       " 'employment',\n",
       " 'of',\n",
       " 'personnel',\n",
       " 'specifically',\n",
       " 'dedicated',\n",
       " 'to',\n",
       " 'pharmacovigilance',\n",
       " 'is',\n",
       " 'a',\n",
       " 'successful',\n",
       " 'strategy',\n",
       " 'to',\n",
       " 'improve',\n",
       " 'the',\n",
       " 'number',\n",
       " 'and',\n",
       " 'quality',\n",
       " 'of',\n",
       " 'adr',\n",
       " 'reports',\n",
       " 'farmarel',\n",
       " 'the',\n",
       " 'first',\n",
       " 'programme',\n",
       " 'of',\n",
       " 'active',\n",
       " 'pharmacovigilance',\n",
       " 'in',\n",
       " 'oncohaematologic',\n",
       " 'patients',\n",
       " 'significantly',\n",
       " 'contributed',\n",
       " 'to',\n",
       " 'reach',\n",
       " 'the',\n",
       " 'who',\n",
       " 'gold',\n",
       " 'standard',\n",
       " 'for',\n",
       " 'pharmacovigilance',\n",
       " 'in',\n",
       " 'lombardy',\n",
       " 'italy']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example of our tokenizer\n",
    "Tokenize_text_value(data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the ***bag-of-words*** method we can transform any document to a vector. Using this method you have **one column per word and one row per document** and either a binary value 1 if the word is present in a certain document, 0 if not or a count value of the number of times the word appears in the document. \n",
    "\n",
    "For instance, the following three sentences:\n",
    "1. Intelligent applications creates intelligent business processes\n",
    "2. Bots are  intelligent applications\n",
    "3. I do business intelligence\n",
    "\n",
    "Can be represented in the following matrix using the counts of each word as values in the matrix\n",
    "![matrix](http://www.darrinbishop.com/wp-content/uploads/2017/10/Document-Term-Matrix.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-26T10:57:53.326347Z",
     "start_time": "2018-06-26T10:57:53.098148Z"
    }
   },
   "outputs": [],
   "source": [
    "# transform non-processed data to nummeric features:\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "binary_vectorizer = TfidfVectorizer(input=u'content', analyzer=u'word', binary = True,\n",
    "                                           tokenizer=Tokenize_text_value)  # initialize the binary vectorizer\n",
    "count_vectorizer = TfidfVectorizer(input=u'content', analyzer=u'word', use_idf=False, \n",
    "                                           tokenizer=Tokenize_text_value)  # initialize the count vectorizer\n",
    "\n",
    "binary_matrix = binary_vectorizer.fit_transform(data)  # fit & transform\n",
    "count_matrix = count_vectorizer.fit_transform(data)  # fit & transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-26T10:57:54.192171Z",
     "start_time": "2018-06-26T10:57:54.184676Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(743, 10098)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check our output matrix shape: rows = documents, columns = words\n",
    "binary_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check performance in a basic model\n",
    "We'll apply now a model on our 2 matrices. For this we will use the ***Naive Bayes model***, which (as the name tells) is based on the probabilistic Bayes theorem. It is used a lot in text-mining as it is really **fast** to train and apply and is able to **handle a lot of features**, which is often the case in text-mining, when you have one column per word. We will use the ***kappa*** measure to evaluate model performance. Kappa is a metric that is robust to class-imbalances in the data and varies from -1 to +1 with 0 being a random performance and +1 a perfect performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-26T11:00:09.139812Z",
     "start_time": "2018-06-26T11:00:09.131285Z"
    }
   },
   "outputs": [],
   "source": [
    "# apply cross validation Naive Bayes model\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import (cohen_kappa_score, make_scorer)\n",
    "\n",
    "NB = MultinomialNB() # our Naive Bayes Model initialisation\n",
    "scorer = make_scorer(cohen_kappa_score) # Our kappa score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-26T11:00:13.463762Z",
     "start_time": "2018-06-26T11:00:13.423681Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross validation on Binary matrix with a mean kappa score of 0.210382 and variance of 0.003118\n"
     ]
    }
   ],
   "source": [
    "scores = cross_val_score(NB,binary_matrix,labels,scoring = scorer,cv = 5 )\n",
    "print('Cross validation on Binary matrix with a mean kappa score of %f and variance of %f' % (scores.mean(),scores.var()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-26T11:00:13.957971Z",
     "start_time": "2018-06-26T11:00:13.921384Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross validation on Count matrix with a mean kappa score of 0.064193 and variance of 0.000682\n"
     ]
    }
   ],
   "source": [
    "scores = cross_val_score(NB,count_matrix,labels,scoring = scorer,cv = 5 )\n",
    "print('Cross validation on Count matrix with a mean kappa score of %f and variance of %f' % (scores.mean(),scores.var()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF transformation\n",
    "\n",
    "An alternative to the binary and count matrix is the **tf-idf transformation**. It stands for ***Term Frequency - Inverse Document Frequency*** and is a measure that will try to find the words that are unique to each document and that characterizes the document compared to the other documents. How this achieved is by taking the term frequency (which is the same as the count that we have defined before) and multiplying it by the inverse document frequency (which is low when the term appears in all other documents and high when it appears in few other documents):\n",
    "\n",
    "![TF-IDF](https://chrisalbon.com/images/machine_learning_flashcards/TF-IDF_print.png)\n",
    "*Copyright © Chris Albon, 2018*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-26T11:02:25.579963Z",
     "start_time": "2018-06-26T11:02:25.444010Z"
    }
   },
   "outputs": [],
   "source": [
    "# transform non-processed data to nummeric features:\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(input=u'content', analyzer=u'word', use_idf=True, smooth_idf = True ,\n",
    "                                           tokenizer=Tokenize_text_value)  # initialize the tf-idf vectorizer\n",
    "\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(data)  # fit & transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-26T11:02:29.368497Z",
     "start_time": "2018-06-26T11:02:29.332897Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross validation on TF-IDF matrix with a mean kappa score of 0.320332 and variance of 0.003960\n"
     ]
    }
   ],
   "source": [
    "scores = cross_val_score(NB,tfidf_matrix,labels,scoring = scorer,cv = 5 )\n",
    "print('Cross validation on TF-IDF matrix with a mean kappa score of %f and variance of %f' % (scores.mean(),scores.var()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to improve this score? \n",
    "How come the TF-IDF works the best, followed closely by the binary matrix and with the count matrix far behind? Let's have a look at the words that occur the most in the different documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-26T11:03:24.639497Z",
     "start_time": "2018-06-26T11:03:24.618407Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Find words with maximum occurence for each document in the count_matrix\n",
    "max_counts_per_doc = np.asarray(np.argmax(count_matrix,axis = 1)).ravel()\n",
    "# Count how many times every word is the most occuring word across all documents\n",
    "unique, counts = np.unique(max_counts_per_doc,return_counts=True)\n",
    "# Keep only the words that are the most frequent word of at least 5 different documents\n",
    "frequent = unique[counts > 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-26T11:03:39.332751Z",
     "start_time": "2018-06-26T11:03:39.310693Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\n",
      "and\n",
      "for\n",
      "in\n",
      "of\n",
      "the\n",
      "to\n",
      "with\n"
     ]
    }
   ],
   "source": [
    "# Retrieve the vocabulary of our count matrix\n",
    "vocab = count_vectorizer.get_feature_names()\n",
    "# print out the words in frequent\n",
    "for i in frequent:\n",
    "    print(vocab[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see those words are all words without any added value as they are mostly used to link certain words together in sentences, but have no standalone value. This is what we call ***Stop words***. So knowing that, we can find an intuition of why the tf-idf and binary transformations worked better than the count one. In the count one, we have seen that words that appear a lot, but have no value as such, get a high weight/value, whereas in binary every word gets the same weight and in tf-idf, the words that appear a lot in the other documents are automatically given a lower weight thanks to the IDF part. To avoid this problem we usually remove stop words\n",
    "\n",
    "### Removing Stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-26T11:05:52.810186Z",
     "start_time": "2018-06-26T11:05:52.491751Z"
    }
   },
   "outputs": [],
   "source": [
    "# Remove the stop words\n",
    "binary_vectorizer = TfidfVectorizer(input=u'content', analyzer=u'word', binary = True,\n",
    "                                           tokenizer=Tokenize_text_value, stop_words = 'english')  # initialize the binary vectorizer\n",
    "count_vectorizer = TfidfVectorizer(input=u'content', analyzer=u'word', use_idf=False, smooth_idf=False,\n",
    "                                           tokenizer=Tokenize_text_value, stop_words = 'english')  # initialize the count vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(input=u'content', analyzer=u'word', use_idf=True, smooth_idf=True,\n",
    "                                           tokenizer=Tokenize_text_value, stop_words = 'english')  # initialize the tf-idf vectorizer\n",
    "binary_matrix = binary_vectorizer.fit_transform(data)  # fit & transform\n",
    "count_matrix = count_vectorizer.fit_transform(data)  # fit & transform\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(data)  # fit & transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-26T11:05:54.843660Z",
     "start_time": "2018-06-26T11:05:54.810572Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross validation on Binary matrix by removing stop-words with a mean kappa score of 0.472072 and variance of 0.001247\n"
     ]
    }
   ],
   "source": [
    "scores = cross_val_score(NB,binary_matrix,labels,scoring = scorer,cv = 5 )\n",
    "print('Cross validation on Binary matrix by removing stop-words with a mean kappa score of %f and variance of %f' % (scores.mean(),scores.var()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-26T11:05:56.129692Z",
     "start_time": "2018-06-26T11:05:56.097102Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross validation on Count matrix by removing stop-words with a mean kappa score of 0.753020 and variance of 0.009759\n"
     ]
    }
   ],
   "source": [
    "scores = cross_val_score(NB,count_matrix,labels,scoring = scorer,cv = 5 )\n",
    "print('Cross validation on Count matrix by removing stop-words with a mean kappa score of %f and variance of %f' % (scores.mean(),scores.var()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-26T11:05:56.129692Z",
     "start_time": "2018-06-26T11:05:56.097102Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross validation on TF-IDF matrix by removing stop-words with a mean kappa score of 0.682766 and variance of 0.011562\n"
     ]
    }
   ],
   "source": [
    "scores = cross_val_score(NB,tfidf_matrix,labels,scoring = scorer,cv = 5 )\n",
    "print('Cross validation on TF-IDF matrix by removing stop-words with a mean kappa score of %f and variance of %f' % (scores.mean(),scores.var()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a big improvement in our performance when we remove the stop words. How can we go a step further? Now the following steps are mostly domain dependent. You have to think about your problem and what you would need to solve it. In this case, if we are using only the abstracts and the titles, if we had to do it ourselves, we would have a look at the most common keywords you have in the articles about Pharmacovigilance and when we have a new article to classify, we would look if we find those same keywords back. However, here we are analyzing all words (minus the stopwords) and not only the keywords. So we could try to filter out to keep only words that appear at least a certain number of times across all documents.\n",
    "### Keeping only key-words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-26T11:07:37.725951Z",
     "start_time": "2018-06-26T11:07:37.399486Z"
    }
   },
   "outputs": [],
   "source": [
    "# keep only words that appear at least in 5% of the documents:\n",
    "binary_vectorizer = TfidfVectorizer(input=u'content', analyzer=u'word', binary = True,\n",
    "                                           tokenizer=Tokenize_text_value, stop_words = 'english'\n",
    "                                        , min_df = 0.05)  # initialize the binary vectorizer\n",
    "count_vectorizer = TfidfVectorizer(input=u'content', analyzer=u'word', use_idf=False, smooth_idf=False,\n",
    "                                           tokenizer=Tokenize_text_value, stop_words = 'english'\n",
    "                                       , min_df = 0.05)  # initialize the count vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(input=u'content', analyzer=u'word', use_idf=True, smooth_idf=True,\n",
    "                                           tokenizer=Tokenize_text_value, stop_words = 'english'\n",
    "                                       , min_df = 0.05)  # initialize the tf-idf vectorizer\n",
    "binary_matrix = binary_vectorizer.fit_transform(data)  # fit & transform\n",
    "count_matrix = count_vectorizer.fit_transform(data)  # fit & transform\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(data)  # fit & transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-26T11:07:38.601172Z",
     "start_time": "2018-06-26T11:07:38.575631Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross validation on Binary matrix by keeping only keywords appearing in at least 5% of the documents with a mean kappa score of 0.876158 and variance of 0.003158\n"
     ]
    }
   ],
   "source": [
    "scores = cross_val_score(NB,binary_matrix,labels,scoring = scorer,cv = 5 )\n",
    "print('Cross validation on Binary matrix by keeping only keywords appearing in at least 5%% of the documents with a mean kappa score of %f and variance of %f' % (scores.mean(),scores.var()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-26T11:07:39.163056Z",
     "start_time": "2018-06-26T11:07:39.135983Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross validation on Count matrix by keeping only keywords appearing in at least 5% of the documents with a mean kappa score of 0.951631 and variance of 0.001133\n"
     ]
    }
   ],
   "source": [
    "scores = cross_val_score(NB,count_matrix,labels,scoring = scorer,cv = 5 )\n",
    "print('Cross validation on Count matrix by keeping only keywords appearing in at least 5%% of the documents with a mean kappa score of %f and variance of %f' % (scores.mean(),scores.var()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-26T11:07:39.747876Z",
     "start_time": "2018-06-26T11:07:39.719775Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross validation on TF-IDF matrix by keeping only keywords appearing in at least 5% of the documents with a mean kappa score of 0.916734 and variance of 0.001633\n"
     ]
    }
   ],
   "source": [
    "scores = cross_val_score(NB,tfidf_matrix,labels,scoring = scorer,cv = 5 )\n",
    "print('Cross validation on TF-IDF matrix by keeping only keywords appearing in at least 5%% of the documents with a mean kappa score of %f and variance of %f' % (scores.mean(),scores.var()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final improvements\n",
    "We've made a big improvement with this one as well. We can even go further and add some extra fine-tunings. Let's have a look at the final key-words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-26T11:08:08.329460Z",
     "start_time": "2018-06-26T11:08:08.318461Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['active',\n",
       " 'activities',\n",
       " 'activity',\n",
       " 'administration',\n",
       " 'adrs',\n",
       " 'adverse',\n",
       " 'aim',\n",
       " 'analysis',\n",
       " 'anti',\n",
       " 'approach',\n",
       " 'approaches',\n",
       " 'approved',\n",
       " 'article',\n",
       " 'assess',\n",
       " 'assessment',\n",
       " 'associated',\n",
       " 'available',\n",
       " 'based',\n",
       " 'big',\n",
       " 'care',\n",
       " 'case',\n",
       " 'challenges',\n",
       " 'chronic',\n",
       " 'clinical',\n",
       " 'clinics',\n",
       " 'companies',\n",
       " 'company',\n",
       " 'compared',\n",
       " 'conducted',\n",
       " 'control',\n",
       " 'controlled',\n",
       " 'current',\n",
       " 'currently',\n",
       " 'd',\n",
       " 'data',\n",
       " 'database',\n",
       " 'design',\n",
       " 'detection',\n",
       " 'developed',\n",
       " 'developing',\n",
       " 'development',\n",
       " 'diabetes',\n",
       " 'different',\n",
       " 'discovery',\n",
       " 'disease',\n",
       " 'diseases',\n",
       " 'dose',\n",
       " 'drug',\n",
       " 'drugs',\n",
       " 'effect',\n",
       " 'effective',\n",
       " 'effects',\n",
       " 'efficacy',\n",
       " 'european',\n",
       " 'evaluate',\n",
       " 'evaluation',\n",
       " 'events',\n",
       " 'evidence',\n",
       " 'factors',\n",
       " 'following',\n",
       " 'future',\n",
       " 'global',\n",
       " 'health',\n",
       " 'healthcare',\n",
       " 'high',\n",
       " 'human',\n",
       " 'identify',\n",
       " 'impact',\n",
       " 'important',\n",
       " 'improve',\n",
       " 'including',\n",
       " 'increase',\n",
       " 'increased',\n",
       " 'industry',\n",
       " 'information',\n",
       " 'inhibitor',\n",
       " 'international',\n",
       " 'issues',\n",
       " 'key',\n",
       " 'knowledge',\n",
       " 'known',\n",
       " 'label',\n",
       " 'large',\n",
       " 'life',\n",
       " 'limited',\n",
       " 'long',\n",
       " 'major',\n",
       " 'management',\n",
       " 'market',\n",
       " 'marketing',\n",
       " 'medical',\n",
       " 'medication',\n",
       " 'medicine',\n",
       " 'medicines',\n",
       " 'method',\n",
       " 'methods',\n",
       " 'mg',\n",
       " 'model',\n",
       " 'monitoring',\n",
       " 'multiple',\n",
       " 'national',\n",
       " 'need',\n",
       " 'new',\n",
       " 'non',\n",
       " 'novel',\n",
       " 'number',\n",
       " 'open',\n",
       " 'oral',\n",
       " 'paper',\n",
       " 'patient',\n",
       " 'patients',\n",
       " 'pharma',\n",
       " 'pharmaceutical',\n",
       " 'pharmacovigilance',\n",
       " 'phase',\n",
       " 'post',\n",
       " 'potential',\n",
       " 'practice',\n",
       " 'present',\n",
       " 'process',\n",
       " 'product',\n",
       " 'products',\n",
       " 'profile',\n",
       " 'provide',\n",
       " 'public',\n",
       " 'quality',\n",
       " 'reactions',\n",
       " 'recent',\n",
       " 'regulatory',\n",
       " 'related',\n",
       " 'report',\n",
       " 'reported',\n",
       " 'reporting',\n",
       " 'reports',\n",
       " 'research',\n",
       " 'results',\n",
       " 'review',\n",
       " 'risk',\n",
       " 'role',\n",
       " 's',\n",
       " 'safety',\n",
       " 'scientific',\n",
       " 'significant',\n",
       " 'specific',\n",
       " 'spontaneous',\n",
       " 'studies',\n",
       " 'study',\n",
       " 'systems',\n",
       " 'technology',\n",
       " 'term',\n",
       " 'therapeutic',\n",
       " 'therapy',\n",
       " 'time',\n",
       " 'treatment',\n",
       " 'trial',\n",
       " 'trials',\n",
       " 'type',\n",
       " 'use',\n",
       " 'used',\n",
       " 'using',\n",
       " 'world',\n",
       " 'years']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that some words all refer to the same thing: *report, reported, reporting, reports* all refer to one same thing *report* and should therefore be grouped together => this can be done by ***stemming***\n",
    "### Stemming\n",
    "Stemming is a technique where we try to reduce words to a common base form, this is done by chopping off the last part of the word: s's are removed, -ing is removed, -ed is removed, ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-26T11:09:05.228377Z",
     "start_time": "2018-06-26T11:09:05.221857Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define a stemmer that will preprocess the text before transforming it\n",
    "from nltk.stem.porter import PorterStemmer  \n",
    "def preprocess(value):   \n",
    "    stemmer = PorterStemmer() \n",
    "     #split in tokens\n",
    "    return ' '.join([stemmer.stem(i) for i in Tokenize_text_value(value) ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-26T11:09:07.213069Z",
     "start_time": "2018-06-26T11:09:07.197026Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "farmarel an italian pharmacovigilance project to monitor and evaluate adverse drug reactions in haematologic patients adverse drug reactions adrs reduce patients quality of life increase mortality and morbidity and have a negative economic impact on healthcare systems nevertheless the importance of adr reporting is often underestimated the project farmarel has been developed to monitor and evaluate adrs in haematological patients and to increase pharmacovigilance culture among haematology specialists in haematology units based in lombardy italy a dedicated specialist with the task of encouraging adrs reporting and sensitizing healthcare professionals to pharmacovigilance has been assigned the adrs occurring in haematological patients were collected electronically and then analysed with multiple logistic regression between january and december reports were collected the number of adrs was higher in older adults in male and in non hodgkin lymphoma patients most reactions were severe required or prolonged hospitalization but in most cases they were fully resolved at the time of reporting according to schumock and thornton criteria a percentage of adrs as high as was found to be preventable versus according to reporter opinion patients haematological diagnosis not age or gender resulted to be the variable that most influenced adr in particular severity and outcome the employment of personnel specifically dedicated to pharmacovigilance is a successful strategy to improve the number and quality of adr reports farmarel the first programme of active pharmacovigilance in oncohaematologic patients significantly contributed to reach the who gold standard for pharmacovigilance in lombardy italy\n",
      "\n",
      "\n",
      "farmarel an italian pharmacovigil project to monitor and evalu advers drug reaction in haematolog patient advers drug reaction adr reduc patient qualiti of life increas mortal and morbid and have a neg econom impact on healthcar system nevertheless the import of adr report is often underestim the project farmarel ha been develop to monitor and evalu adr in haematolog patient and to increas pharmacovigil cultur among haematolog specialist in haematolog unit base in lombardi itali a dedic specialist with the task of encourag adr report and sensit healthcar profession to pharmacovigil ha been assign the adr occur in haematolog patient were collect electron and then analys with multipl logist regress between januari and decemb report were collect the number of adr wa higher in older adult in male and in non hodgkin lymphoma patient most reaction were sever requir or prolong hospit but in most case they were fulli resolv at the time of report accord to schumock and thornton criteria a percentag of adr as high as wa found to be prevent versu accord to report opinion patient haematolog diagnosi not age or gender result to be the variabl that most influenc adr in particular sever and outcom the employ of personnel specif dedic to pharmacovigil is a success strategi to improv the number and qualiti of adr report farmarel the first programm of activ pharmacovigil in oncohaematolog patient significantli contribut to reach the who gold standard for pharmacovigil in lombardi itali\n"
     ]
    }
   ],
   "source": [
    "# Have a look at what it gives on the first article\n",
    "print(' '.join([i for i in Tokenize_text_value(data[0]) ])) # original\n",
    "print('\\n')\n",
    "print(preprocess(data[0])) #stemmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-26T11:10:06.324049Z",
     "start_time": "2018-06-26T11:09:59.126844Z"
    }
   },
   "outputs": [],
   "source": [
    "# Preprocess the documents by stemming the words\n",
    "binary_vectorizer = TfidfVectorizer(input=u'content', analyzer=u'word', binary = True,\n",
    "                                           tokenizer=Tokenize_text_value, stop_words = 'english'\n",
    "                                        , min_df = 0.05, preprocessor = preprocess)  # initialize the binary vectorizer\n",
    "count_vectorizer = TfidfVectorizer(input=u'content', analyzer=u'word', use_idf=False, smooth_idf=False,\n",
    "                                           tokenizer=Tokenize_text_value, stop_words = 'english'\n",
    "                                       , min_df = 0.05, preprocessor = preprocess)  # initialize the count vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(input=u'content', analyzer=u'word', use_idf=True, smooth_idf=True,\n",
    "                                           tokenizer=Tokenize_text_value, stop_words = 'english'\n",
    "                                       , min_df = 0.05, preprocessor = preprocess)  # initialize the tf-idf vectorizer\n",
    "binary_matrix = binary_vectorizer.fit_transform(data)  # fit & transform\n",
    "count_matrix = count_vectorizer.fit_transform(data)  # fit & transform\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(data)  # fit & transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-26T11:10:06.380225Z",
     "start_time": "2018-06-26T11:10:06.330569Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross validation on Binary matrix by stemming and keeping only keywords appearing in at least 10% of the documents with a mean kappa score of 0.868418 and variance of 0.000270\n"
     ]
    }
   ],
   "source": [
    "scores = cross_val_score(NB,binary_matrix,labels,scoring = scorer,cv = 5 )\n",
    "print('Cross validation on Binary matrix by stemming and keeping only keywords appearing in at least 10%% of the documents with a mean kappa score of %f and variance of %f' % (scores.mean(),scores.var()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-26T11:10:06.421811Z",
     "start_time": "2018-06-26T11:10:06.385717Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross validation on Count matrix by stemming and keeping only keywords appearing in at least 10% of the documents with a mean kappa score of 0.944600 and variance of 0.001127\n"
     ]
    }
   ],
   "source": [
    "scores = cross_val_score(NB,count_matrix,labels,scoring = scorer,cv = 5 )\n",
    "print('Cross validation on Count matrix by stemming and keeping only keywords appearing in at least 10%% of the documents with a mean kappa score of %f and variance of %f' % (scores.mean(),scores.var()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-26T11:10:06.462920Z",
     "start_time": "2018-06-26T11:10:06.425821Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross validation on TF-IDF matrix by stemming and keeping only keywords appearing in at least 10% of the documents with a mean kappa score of 0.901758 and variance of 0.001610\n"
     ]
    }
   ],
   "source": [
    "scores = cross_val_score(NB,tfidf_matrix,labels,scoring = scorer,cv = 5 )\n",
    "print('Cross validation on TF-IDF matrix by stemming and keeping only keywords appearing in at least 10%% of the documents with a mean kappa score of %f and variance of %f' % (scores.mean(),scores.var()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-26T11:10:06.486485Z",
     "start_time": "2018-06-26T11:10:06.467952Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['activ',\n",
       " 'addit',\n",
       " 'administr',\n",
       " 'adr',\n",
       " 'advanc',\n",
       " 'advers',\n",
       " 'agent',\n",
       " 'aim',\n",
       " 'analysi',\n",
       " 'anti',\n",
       " 'applic',\n",
       " 'approach',\n",
       " 'approv',\n",
       " 'articl',\n",
       " 'assess',\n",
       " 'associ',\n",
       " 'author',\n",
       " 'avail',\n",
       " 'base',\n",
       " 'becom',\n",
       " 'benefit',\n",
       " 'big',\n",
       " 'biolog',\n",
       " 'care',\n",
       " 'case',\n",
       " 'caus',\n",
       " 'challeng',\n",
       " 'chang',\n",
       " 'chronic',\n",
       " 'clinic',\n",
       " 'collabor',\n",
       " 'combin',\n",
       " 'commerci',\n",
       " 'commun',\n",
       " 'compani',\n",
       " 'compar',\n",
       " 'compound',\n",
       " 'concern',\n",
       " 'conduct',\n",
       " 'consid',\n",
       " 'control',\n",
       " 'cost',\n",
       " 'current',\n",
       " 'd',\n",
       " 'data',\n",
       " 'databas',\n",
       " 'demonstr',\n",
       " 'describ',\n",
       " 'design',\n",
       " 'detect',\n",
       " 'determin',\n",
       " 'develop',\n",
       " 'diabet',\n",
       " 'differ',\n",
       " 'discoveri',\n",
       " 'discuss',\n",
       " 'diseas',\n",
       " 'dose',\n",
       " 'drug',\n",
       " 'dure',\n",
       " 'effect',\n",
       " 'efficaci',\n",
       " 'emerg',\n",
       " 'establish',\n",
       " 'european',\n",
       " 'evalu',\n",
       " 'event',\n",
       " 'evid',\n",
       " 'exist',\n",
       " 'factor',\n",
       " 'follow',\n",
       " 'formul',\n",
       " 'function',\n",
       " 'futur',\n",
       " 'gener',\n",
       " 'global',\n",
       " 'group',\n",
       " 'ha',\n",
       " 'health',\n",
       " 'healthcar',\n",
       " 'help',\n",
       " 'high',\n",
       " 'howev',\n",
       " 'human',\n",
       " 'identifi',\n",
       " 'impact',\n",
       " 'implement',\n",
       " 'import',\n",
       " 'improv',\n",
       " 'includ',\n",
       " 'increas',\n",
       " 'indic',\n",
       " 'industri',\n",
       " 'inform',\n",
       " 'inhibitor',\n",
       " 'initi',\n",
       " 'innov',\n",
       " 'institut',\n",
       " 'integr',\n",
       " 'intern',\n",
       " 'investig',\n",
       " 'involv',\n",
       " 'issu',\n",
       " 'key',\n",
       " 'knowledg',\n",
       " 'known',\n",
       " 'label',\n",
       " 'larg',\n",
       " 'lead',\n",
       " 'level',\n",
       " 'life',\n",
       " 'limit',\n",
       " 'long',\n",
       " 'major',\n",
       " 'make',\n",
       " 'manag',\n",
       " 'mani',\n",
       " 'market',\n",
       " 'medic',\n",
       " 'medicin',\n",
       " 'method',\n",
       " 'mg',\n",
       " 'model',\n",
       " 'monitor',\n",
       " 'month',\n",
       " 'multipl',\n",
       " 'nation',\n",
       " 'need',\n",
       " 'new',\n",
       " 'non',\n",
       " 'novel',\n",
       " 'number',\n",
       " 'object',\n",
       " 'observ',\n",
       " 'onli',\n",
       " 'open',\n",
       " 'opportun',\n",
       " 'oral',\n",
       " 'organ',\n",
       " 'outcom',\n",
       " 'paper',\n",
       " 'patent',\n",
       " 'patient',\n",
       " 'perform',\n",
       " 'period',\n",
       " 'pharma',\n",
       " 'pharmaceut',\n",
       " 'pharmacovigil',\n",
       " 'phase',\n",
       " 'popul',\n",
       " 'posit',\n",
       " 'post',\n",
       " 'potenti',\n",
       " 'practic',\n",
       " 'present',\n",
       " 'prevent',\n",
       " 'problem',\n",
       " 'process',\n",
       " 'product',\n",
       " 'profession',\n",
       " 'profil',\n",
       " 'progress',\n",
       " 'provid',\n",
       " 'public',\n",
       " 'qualiti',\n",
       " 'rate',\n",
       " 'reaction',\n",
       " 'receiv',\n",
       " 'recent',\n",
       " 'receptor',\n",
       " 'recommend',\n",
       " 'reduc',\n",
       " 'regard',\n",
       " 'regul',\n",
       " 'regulatori',\n",
       " 'relat',\n",
       " 'remain',\n",
       " 'report',\n",
       " 'requir',\n",
       " 'research',\n",
       " 'respons',\n",
       " 'result',\n",
       " 'review',\n",
       " 'risk',\n",
       " 'role',\n",
       " 's',\n",
       " 'safeti',\n",
       " 'scienc',\n",
       " 'scientif',\n",
       " 'select',\n",
       " 'set',\n",
       " 'sever',\n",
       " 'share',\n",
       " 'signal',\n",
       " 'signific',\n",
       " 'sinc',\n",
       " 'sourc',\n",
       " 'specif',\n",
       " 'spontan',\n",
       " 'standard',\n",
       " 'state',\n",
       " 'strategi',\n",
       " 'studi',\n",
       " 'success',\n",
       " 'suggest',\n",
       " 'support',\n",
       " 'target',\n",
       " 'technolog',\n",
       " 'term',\n",
       " 'test',\n",
       " 'therapeut',\n",
       " 'therapi',\n",
       " 'thi',\n",
       " 'time',\n",
       " 'treat',\n",
       " 'treatment',\n",
       " 'trial',\n",
       " 'type',\n",
       " 'use',\n",
       " 'valu',\n",
       " 'wa',\n",
       " 'wide',\n",
       " 'work',\n",
       " 'world',\n",
       " 'year']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the stemmed final vocabulary\n",
    "tfidf_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the performance slightly decreases with the stemming. Probably, because now when we are keeping words that appear in only 5% of the documents, we have more words than before, as before words with different endings were counted separately and now they are grouped together. So to correct for this we should increase our 5% threshold to take this effect into account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-26T11:11:05.069134Z",
     "start_time": "2018-06-26T11:10:58.489600Z"
    }
   },
   "outputs": [],
   "source": [
    "# Preprocess the documents by stemming the words and keeping only words that appear in at least 10% of the documents:\n",
    "binary_vectorizer = TfidfVectorizer(input=u'content', analyzer=u'word', binary = True,\n",
    "                                           tokenizer=Tokenize_text_value, stop_words = 'english'\n",
    "                                        , min_df = 0.1, preprocessor = preprocess)  # initialize the binary vectorizer\n",
    "count_vectorizer = TfidfVectorizer(input=u'content', analyzer=u'word', use_idf=False, smooth_idf=False,\n",
    "                                           tokenizer=Tokenize_text_value, stop_words = 'english'\n",
    "                                       , min_df = 0.1, preprocessor = preprocess)  # initialize the count vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(input=u'content', analyzer=u'word', use_idf=True, smooth_idf=True,\n",
    "                                           tokenizer=Tokenize_text_value, stop_words = 'english'\n",
    "                                       , min_df = 0.1, preprocessor = preprocess)  # initialize the tf-idf vectorizer\n",
    "binary_matrix = binary_vectorizer.fit_transform(data)  # fit & transform\n",
    "count_matrix = count_vectorizer.fit_transform(data)  # fit & transform\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(data)  # fit & transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-26T11:11:05.104231Z",
     "start_time": "2018-06-26T11:11:05.071141Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross validation on Binary matrix by stemming with a mean kappa score of 0.893254 and variance of 0.000962\n"
     ]
    }
   ],
   "source": [
    "scores = cross_val_score(NB,binary_matrix,labels,scoring = scorer,cv = 5 )\n",
    "print('Cross validation on Binary matrix by stemming with a mean kappa score of %f and variance of %f' % (scores.mean(),scores.var()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-26T11:11:05.142832Z",
     "start_time": "2018-06-26T11:11:05.108771Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross validation on Count matrix by stemming with a mean kappa score of 0.951618 and variance of 0.001134\n"
     ]
    }
   ],
   "source": [
    "scores = cross_val_score(NB,count_matrix,labels,scoring = scorer,cv = 5 )\n",
    "print('Cross validation on Count matrix by stemming with a mean kappa score of %f and variance of %f' % (scores.mean(),scores.var()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-26T11:11:05.191966Z",
     "start_time": "2018-06-26T11:11:05.147847Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross validation on TF-IDF matrix by stemming with a mean kappa score of 0.937502 and variance of 0.000565\n"
     ]
    }
   ],
   "source": [
    "scores = cross_val_score(NB,tfidf_matrix,labels,scoring = scorer,cv = 5 )\n",
    "print('Cross validation on TF-IDF matrix by stemming with a mean kappa score of %f and variance of %f' % (scores.mean(),scores.var()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-26T11:11:05.206001Z",
     "start_time": "2018-06-26T11:11:05.196977Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['activ',\n",
       " 'advers',\n",
       " 'aim',\n",
       " 'analysi',\n",
       " 'approach',\n",
       " 'assess',\n",
       " 'associ',\n",
       " 'base',\n",
       " 'case',\n",
       " 'challeng',\n",
       " 'clinic',\n",
       " 'compani',\n",
       " 'compar',\n",
       " 'control',\n",
       " 'current',\n",
       " 'data',\n",
       " 'develop',\n",
       " 'discuss',\n",
       " 'diseas',\n",
       " 'drug',\n",
       " 'effect',\n",
       " 'efficaci',\n",
       " 'evalu',\n",
       " 'gener',\n",
       " 'ha',\n",
       " 'health',\n",
       " 'high',\n",
       " 'howev',\n",
       " 'identifi',\n",
       " 'import',\n",
       " 'improv',\n",
       " 'includ',\n",
       " 'increas',\n",
       " 'industri',\n",
       " 'inform',\n",
       " 'investig',\n",
       " 'market',\n",
       " 'medic',\n",
       " 'medicin',\n",
       " 'method',\n",
       " 'need',\n",
       " 'new',\n",
       " 'patient',\n",
       " 'pharma',\n",
       " 'pharmaceut',\n",
       " 'pharmacovigil',\n",
       " 'phase',\n",
       " 'potenti',\n",
       " 'practic',\n",
       " 'present',\n",
       " 'product',\n",
       " 'provid',\n",
       " 'reaction',\n",
       " 'relat',\n",
       " 'report',\n",
       " 'research',\n",
       " 'result',\n",
       " 'review',\n",
       " 'risk',\n",
       " 's',\n",
       " 'safeti',\n",
       " 'studi',\n",
       " 'therapi',\n",
       " 'thi',\n",
       " 'time',\n",
       " 'treatment',\n",
       " 'trial',\n",
       " 'use',\n",
       " 'wa',\n",
       " 'year']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the stemmed final vocabulary\n",
    "tfidf_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have the same or a bit higher performance as before."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
